# ğŸ§± Databricks ETL Pipeline: Auto Loader + Delta Lake + DLT

## ğŸ“Œ Overview
This project demonstrates a modular, production-grade ETL pipeline using **Databricks Auto Loader**, **Delta Lake**, and **Delta Live Tables (DLT)**. It ingests raw data incrementally, applies transformations across Bronze â†’ Silver â†’ Gold layers, and enforces data quality using DLT expectations.

Designed for reusability, governance, and performance optimization, this pipeline is ideal for enterprise-grade ingestion and analytics use cases.

---

## ğŸš€ Features
- âœ… Incremental ingestion with Auto Loader
- ğŸ§ª Data quality enforcement via DLT expectations
- ğŸ—‚ï¸ Medallion architecture: Bronze, Silver, Gold layering
- ğŸ”„ Streaming and batch support
- ğŸ” Unity Catalog-ready for governance
- âš™ï¸ Parameterized for flexible deployment

---

## ğŸ“ Folder Structure
etl-dlt-pipeline/ 
â”œâ”€â”€ notebooks/ 
â”‚ â”œâ”€â”€ etl_dlt_pipeline.py 
â”‚ â””â”€â”€ utils_config.py 
â”œâ”€â”€ sample-data/ 
â”‚ â””â”€â”€ 

---

## ğŸ§ª Pipeline Breakdown

### 1. Bronze Layer: Raw Ingestion
@dlt.table(name="bronze_raw")
def bronze_raw():
    return (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.inferColumnTypes", "true")
        .load("/mnt/raw-data/")
    )

@dlt.table(name="silver_cleaned")
@dlt.expect("valid_id", "id IS NOT NULL")
@dlt.expect_or_drop("valid_timestamp", "timestamp IS NOT NULL")
def silver_cleaned():
    return dlt.read("bronze_raw").dropDuplicates(["id"])

---

### 2. Silver Layer: Cleansing + Validation
@dlt.table(name="silver_cleaned")
@dlt.expect("valid_id", "id IS NOT NULL")
@dlt.expect_or_drop("valid_timestamp", "timestamp IS NOT NULL")
def silver_cleaned():
    return dlt.read("bronze_raw").dropDuplicates(["id"])

---

### 3. Gold Layer: Aggregation
@dlt.table(name="gold_summary")
def gold_summary():
    return dlt.read("silver_cleaned").groupBy("category").agg({"amount": "sum"})

---

### âš™ï¸ Configuration
Use widgets or config files to parameterize:
    - Source path
    - File format
    - Trigger mode (batch vs streaming)

dbutils.widgets.text("source_path", "/mnt/raw-data/")
source_path = dbutils.widgets.get("source_path")

---

### ğŸ“Š Monitoring & Lineage
- View pipeline status in DLT UI
- Track expectations, throughput, and latency
- Visualize lineage across Bronze â†’ Gold

---

### ğŸ” Governance
- Register tables in Unity Catalog
- Apply role-based access controls
- Enable audit logging and schema enforcement

---

### ğŸ§° Requirements
- Databricks Runtime 11.3+ (for DLT support)
- Unity Catalog (optional)
- Sample data in JSON/CSV/Parquet format

---

### ğŸ§  Author Notes
This pipeline is designed for:

    - Hands-on mastery of Databricks ETL
    - Client-ready demos and accelerators
    = Productization into reusable assets

Built by Ramy â€” Senior Data Architect & Engineer focused on high-impact delivery and strategic leverage.

---
