# 🧱 Databricks ETL Pipeline: Auto Loader + Delta Lake + DLT

## 📌 Overview
This project demonstrates a modular, production-grade ETL pipeline using **Databricks Auto Loader**, **Delta Lake**, and **Delta Live Tables (DLT)**. It ingests raw data incrementally, applies transformations across Bronze → Silver → Gold layers, and enforces data quality using DLT expectations.

Designed for reusability, governance, and performance optimization, this pipeline is ideal for enterprise-grade ingestion and analytics use cases.

---

## 🚀 Features
- ✅ Incremental ingestion with Auto Loader
- 🧪 Data quality enforcement via DLT expectations
- 🗂️ Medallion architecture: Bronze, Silver, Gold layering
- 🔄 Streaming and batch support
- 🔐 Unity Catalog-ready for governance
- ⚙️ Parameterized for flexible deployment

---

## 📁 Folder Structure
etl-dlt-pipeline/ 
├── notebooks/ 
│ ├── etl_dlt_pipeline.py 
│ └── utils_config.py 
├── sample-data/ 
│ └── 

---

## 🧪 Pipeline Breakdown

### 1. Bronze Layer: Raw Ingestion
@dlt.table(name="bronze_raw")
def bronze_raw():
    return (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.inferColumnTypes", "true")
        .load("/mnt/raw-data/")
    )

@dlt.table(name="silver_cleaned")
@dlt.expect("valid_id", "id IS NOT NULL")
@dlt.expect_or_drop("valid_timestamp", "timestamp IS NOT NULL")
def silver_cleaned():
    return dlt.read("bronze_raw").dropDuplicates(["id"])

---

### 2. Silver Layer: Cleansing + Validation
@dlt.table(name="silver_cleaned")
@dlt.expect("valid_id", "id IS NOT NULL")
@dlt.expect_or_drop("valid_timestamp", "timestamp IS NOT NULL")
def silver_cleaned():
    return dlt.read("bronze_raw").dropDuplicates(["id"])

---

### 3. Gold Layer: Aggregation
@dlt.table(name="gold_summary")
def gold_summary():
    return dlt.read("silver_cleaned").groupBy("category").agg({"amount": "sum"})

---

### ⚙️ Configuration
Use widgets or config files to parameterize:
    - Source path
    - File format
    - Trigger mode (batch vs streaming)

dbutils.widgets.text("source_path", "/mnt/raw-data/")
source_path = dbutils.widgets.get("source_path")

---

### 📊 Monitoring & Lineage
- View pipeline status in DLT UI
- Track expectations, throughput, and latency
- Visualize lineage across Bronze → Gold

---

### 🔐 Governance
- Register tables in Unity Catalog
- Apply role-based access controls
- Enable audit logging and schema enforcement

---

### 🧰 Requirements
- Databricks Runtime 11.3+ (for DLT support)
- Unity Catalog (optional)
- Sample data in JSON/CSV/Parquet format

---

### 🧠 Author Notes
This pipeline is designed for:

    - Hands-on mastery of Databricks ETL
    - Client-ready demos and accelerators
    = Productization into reusable assets

Built by Ramy — Senior Data Architect & Engineer focused on high-impact delivery and strategic leverage.

---
